{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhiraj36/Malaria-Diagnosis-Using-CNNs/blob/main/Malaria_Detection_using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BXOMvw72y97m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
        "\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset, dataset_info = tfds.load(\n",
        "    'malaria', with_info=True, as_supervised=True, shuffle_files=True, split='train'\n",
        ")\n"
      ],
      "metadata": {
        "id": "W9m9VMT-4ndz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting"
      ],
      "metadata": {
        "id": "KwCtfBCCFibU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
        "\n",
        "  dataset_size=len(dataset)\n",
        "  train_dataset = dataset.take(int(0.6*dataset_size))\n",
        "\n",
        "\n",
        "  val_dataset = dataset.skip(int(TRAIN_RATIO*dataset_size))\n",
        "  val_dataset = val_dataset.take(int(VAL_RATIO*dataset_size))\n",
        "\n",
        "\n",
        "  test_dataset = dataset.skip(int((VAL_RATIO*dataset_size)+(TRAIN_RATIO*dataset_size)))\n",
        "  #print(list(test_dataset.as_numpy_iterator()))\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JQreNcSg5Gi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "#dataset = tf.data.Dataset.range(10)\n",
        "train_dataset, val_dataset, test_dataset = splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
        "\n",
        "#print(list(train_dataset.take(1).as_numpy_iterator()),  list(test_dataset.take(1).as_numpy_iterator()))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a0raGRCU5FP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA VISUALISATION"
      ],
      "metadata": {
        "id": "JG5Lb6hkQjIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(image, label) in enumerate(train_dataset.take(16)):\n",
        "  ax=plt.subplot(4, 4, i+1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(dataset_info.features['label'].int2str(label))\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "mfOMVg0D5tYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_info.features['label'].int2str(0)"
      ],
      "metadata": {
        "id": "vRssHs7GSh87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "2gMLIyKPSs9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Image resizing and then normalisation\n",
        "IM_SIZE = 224\n",
        "def resizing_rescale(image,label):\n",
        "  return tf.image.resize(image,(IM_SIZE, IM_SIZE))/255.0,label\n",
        "train_dataset = train_dataset.map(resizing_rescale)"
      ],
      "metadata": {
        "id": "Msx-7FURSqvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train_dataset.take(1):\n",
        "  print(image, label)"
      ],
      "metadata": {
        "id": "eSkKavnfW67I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.shuffle(buffer_size = 1000, reshuffle_each_iteration =True).batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "tLx37qMjXKN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Creation"
      ],
      "metadata": {
        "id": "dQc8_Nsg8rG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model = tf.keras.Sequential([\n",
        "                     InputLayer(shape=(IM_SIZE, IM_SIZE, 3)),\n",
        "                     Conv2D(filters = 6, kernel_size = 5, strides = 1, padding = 'valid', activation = 'sigmoid'),\n",
        "                     MaxPool2D(pool_size = 2, strides =2),\n",
        "\n",
        "                     Conv2D(filters = 6, kernel_size = 5, strides = 1, padding = 'valid', activation = 'sigmoid'),\n",
        "                     MaxPool2D(pool_size = 2, strides =2),\n",
        "\n",
        "                     Flatten(),\n",
        "\n",
        "                     Dense(1000, activation = \"sigmoid\"),\n",
        "                     Dense(100, activation = \"sigmoid\"),\n",
        "                     Dense(1, activation = \"sigmoid\"),\n",
        "\n",
        "])\n",
        "\n",
        "lenet_model.summary()"
      ],
      "metadata": {
        "id": "j6XQj5YT9Vt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [0,1,0,0]\n",
        "y_pred = [0.6, 0.5, 0.94, 1]\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "bce(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "KUY_JB77N4fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.compile(optimizer=Adam(learning_rate = 0.1),\n",
        "              loss = BinaryCrossentropy(), )\n",
        ""
      ],
      "metadata": {
        "id": "hzGHhQkDOPAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = lenet_model.fit(train_dataset, validation_data = val_dataset, epochs = 100, verbose = 1)"
      ],
      "metadata": {
        "id": "twhQ1Y40Onfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}